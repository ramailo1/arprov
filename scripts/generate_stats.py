#!/usr/bin/env python3
"""
ðŸ“Š Ù…ÙˆÙ„Ø¯ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª CloudStream Extensions Arabic
Ù‡Ø°Ø§ Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬ ÙŠÙˆÙ„Ø¯ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø´Ø§Ù…Ù„Ø© Ø¹Ù† Ø§Ù„Ù…Ø´Ø±ÙˆØ¹
"""

import os
import json
import glob
import re
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any
import yaml

class ProjectStats:
    def __init__(self):
        self.root_dir = Path(".")
        self.stats = {
            "project_info": {},
            "files": {},
            "code": {},
            "providers": {},
            "extractors": {},
            "languages": {},
            "quality": {},
            "activity": {},
            "timestamp": datetime.now().isoformat()
        }
    
    def analyze_project_info(self):
        """ØªØ­Ù„ÙŠÙ„ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…Ø´Ø±ÙˆØ¹ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©"""
        print("ðŸ“‹ ØªØ­Ù„ÙŠÙ„ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…Ø´Ø±ÙˆØ¹...")
        
        # Ù‚Ø±Ø§Ø¡Ø© repo.json
        repo_file = self.root_dir / "repo.json"
        if repo_file.exists():
            with open(repo_file, "r", encoding="utf-8") as f:
                repo_data = json.load(f)
                self.stats["project_info"] = {
                    "name": repo_data.get("name", "CloudStream Extensions Arabic"),
                    "version": repo_data.get("version", "2.0.0"),
                    "description": repo_data.get("description", ""),
                    "author": repo_data.get("author", "dhomred"),
                    "license": repo_data.get("license", "MIT"),
                    "homepage": repo_data.get("homepage", ""),
                    "repository": repo_data.get("repository", {}).get("url", "")
                }
        
        # Ù‚Ø±Ø§Ø¡Ø© package.json
        package_file = self.root_dir / "package.json"
        if package_file.exists():
            with open(package_file, "r", encoding="utf-8") as f:
                package_data = json.load(f)
                self.stats["project_info"].update({
                    "keywords": package_data.get("keywords", []),
                    "dependencies": len(package_data.get("dependencies", {})),
                    "dev_dependencies": len(package_data.get("devDependencies", {}))
                })
    
    def analyze_files(self):
        """ØªØ­Ù„ÙŠÙ„ Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø´Ø±ÙˆØ¹"""
        print("ðŸ“ ØªØ­Ù„ÙŠÙ„ Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø´Ø±ÙˆØ¹...")
        
        file_types = {}
        total_size = 0
        total_files = 0
        
        # Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ù‡Ù…Ø©
        important_extensions = [
            ".kt", ".java", ".py", ".js", ".ts", ".json", ".md", ".yml", ".yaml",
            ".gradle", ".properties", ".xml", ".html", ".css", ".scss", ".sass"
        ]
        
        for ext in important_extensions:
            files = list(self.root_dir.rglob(f"*{ext}"))
            if files:
                count = len(files)
                size = sum(f.stat().st_size for f in files if f.is_file())
                
                file_types[ext] = {
                    "count": count,
                    "size_bytes": size,
                    "size_kb": round(size / 1024, 2),
                    "size_mb": round(size / (1024 * 1024), 2)
                }
                
                total_files += count
                total_size += size
        
        self.stats["files"] = {
            "total_files": total_files,
            "total_size_bytes": total_size,
            "total_size_kb": round(total_size / 1024, 2),
            "total_size_mb": round(total_size / (1024 * 1024), 2),
            "file_types": file_types,
            "largest_files": self.get_largest_files(10)
        }
    
    def get_largest_files(self, limit: int = 10) -> List[Dict[str, Any]]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø£ÙƒØ¨Ø± Ø§Ù„Ù…Ù„ÙØ§Øª"""
        files_with_sizes = []
        
        for file_path in self.root_dir.rglob("*"):
            if file_path.is_file() and not str(file_path).startswith("."):
                try:
                    size = file_path.stat().st_size
                    files_with_sizes.append({
                        "path": str(file_path),
                        "size_bytes": size,
                        "size_kb": round(size / 1024, 2),
                        "size_mb": round(size / (1024 * 1024), 2)
                    })
                except (OSError, PermissionError):
                    continue
        
        # ØªØ±ØªÙŠØ¨ Ø­Ø³Ø¨ Ø§Ù„Ø­Ø¬Ù…
        files_with_sizes.sort(key=lambda x: x["size_bytes"], reverse=True)
        return files_with_sizes[:limit]
    
    def analyze_code(self):
        """ØªØ­Ù„ÙŠÙ„ Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„Ù…ØµØ¯Ø±ÙŠ"""
        print("ðŸ’» ØªØ­Ù„ÙŠÙ„ Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„Ù…ØµØ¯Ø±ÙŠ...")
        
        code_stats = {
            "total_lines": 0,
            "total_files": 0,
            "languages": {},
            "code_quality": {}
        }
        
        # ØªØ­Ù„ÙŠÙ„ Ù…Ù„ÙØ§Øª Kotlin
        kt_files = list(self.root_dir.rglob("*.kt"))
        if kt_files:
            kt_stats = self.analyze_kotlin_files(kt_files)
            code_stats["languages"]["kotlin"] = kt_stats
        
        # ØªØ­Ù„ÙŠÙ„ Ù…Ù„ÙØ§Øª Python
        py_files = list(self.root_dir.rglob("*.py"))
        if py_files:
            py_stats = self.analyze_python_files(py_files)
            code_stats["languages"]["python"] = py_stats
        
        # ØªØ­Ù„ÙŠÙ„ Ù…Ù„ÙØ§Øª Java
        java_files = list(self.root_dir.rglob("*.java"))
        if java_files:
            java_stats = self.analyze_java_files(java_files)
            code_stats["languages"]["java"] = java_stats
        
        # Ø­Ø³Ø§Ø¨ Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠ
        for lang_stats in code_stats["languages"].values():
            code_stats["total_lines"] += lang_stats.get("total_lines", 0)
            code_stats["total_files"] += lang_stats.get("total_files", 0)
        
        self.stats["code"] = code_stats
    
    def analyze_kotlin_files(self, files: List[Path]) -> Dict[str, Any]:
        """ØªØ­Ù„ÙŠÙ„ Ù…Ù„ÙØ§Øª Kotlin"""
        stats = {
            "total_files": len(files),
            "total_lines": 0,
            "total_classes": 0,
            "total_functions": 0,
            "total_comments": 0,
            "complexity": 0
        }
        
        for file_path in files:
            try:
                with open(file_path, "r", encoding="utf-8") as f:
                    content = f.read()
                    lines = content.split("\n")
                    
                    stats["total_lines"] += len(lines)
                    stats["total_classes"] += len(re.findall(r"\bclass\s+\w+", content))
                    stats["total_functions"] += len(re.findall(r"\bfun\s+\w+", content))
                    stats["total_comments"] += len(re.findall(r"//.*$", content, re.MULTILINE))
                    stats["total_comments"] += len(re.findall(r"/\*.*?\*/", content, re.DOTALL))
                    
            except (UnicodeDecodeError, PermissionError):
                continue
        
        return stats
    
    def analyze_python_files(self, files: List[Path]) -> Dict[str, Any]:
        """ØªØ­Ù„ÙŠÙ„ Ù…Ù„ÙØ§Øª Python"""
        stats = {
            "total_files": len(files),
            "total_lines": 0,
            "total_classes": 0,
            "total_functions": 0,
            "total_comments": 0,
            "total_imports": 0,
            "complexity": 0
        }
        
        for file_path in files:
            try:
                with open(file_path, "r", encoding="utf-8") as f:
                    content = f.read()
                    lines = content.split("\n")
                    
                    stats["total_lines"] += len(lines)
                    stats["total_classes"] += len(re.findall(r"^class\s+\w+", content, re.MULTILINE))
                    stats["total_functions"] += len(re.findall(r"^def\s+\w+", content, re.MULTILINE))
                    stats["total_comments"] += len(re.findall(r"#.*$", content, re.MULTILINE))
                    stats["total_comments"] += len(re.findall(r'""".*?"""', content, re.DOTALL))
                    stats["total_imports"] += len(re.findall(r"^(import|from)\s+", content, re.MULTILINE))
                    
            except (UnicodeDecodeError, PermissionError):
                continue
        
        return stats
    
    def analyze_java_files(self, files: List[Path]) -> Dict[str, Any]:
        """ØªØ­Ù„ÙŠÙ„ Ù…Ù„ÙØ§Øª Java"""
        stats = {
            "total_files": len(files),
            "total_lines": 0,
            "total_classes": 0,
            "total_methods": 0,
            "total_comments": 0,
            "complexity": 0
        }
        
        for file_path in files:
            try:
                with open(file_path, "r", encoding="utf-8") as f:
                    content = f.read()
                    lines = content.split("\n")
                    
                    stats["total_lines"] += len(lines)
                    stats["total_classes"] += len(re.findall(r"\bclass\s+\w+", content))
                    stats["total_methods"] += len(re.findall(r"\b(public|private|protected)\s+.*\w+\s*\(", content))
                    stats["total_comments"] += len(re.findall(r"//.*$", content, re.MULTILINE))
                    stats["total_comments"] += len(re.findall(r"/\*.*?\*/", content, re.DOTALL))
                    
            except (UnicodeDecodeError, PermissionError):
                continue
        
        return stats
    
    def analyze_providers(self):
        """ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø²ÙˆØ¯ÙŠÙ†"""
        print("ðŸ“º ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø²ÙˆØ¯ÙŠÙ†...")
        
        providers = []
        
        # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø²ÙˆØ¯ÙŠÙ†
        provider_files = list(self.root_dir.rglob("*Provider*.kt")) + \
                        list(self.root_dir.rglob("*Provider*.java")) + \
                        list(self.root_dir.rglob("*provider*.py"))
        
        for file_path in provider_files:
            try:
                with open(file_path, "r", encoding="utf-8") as f:
                    content = f.read()
                    
                    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…Ø²ÙˆØ¯
                    provider_info = {
                        "file": str(file_path),
                        "name": file_path.stem,
                        "type": self.detect_provider_type(content),
                        "features": self.extract_provider_features(content),
                        "domains": self.extract_domains(content)
                    }
                    
                    providers.append(provider_info)
                    
            except (UnicodeDecodeError, PermissionError):
                continue
        
        self.stats["providers"] = {
            "total_count": len(providers),
            "providers": providers,
            "by_type": self.categorize_providers(providers)
        }
    
    def detect_provider_type(self, content: str) -> str:
        """Ø§ÙƒØªØ´Ø§Ù Ù†ÙˆØ¹ Ø§Ù„Ù…Ø²ÙˆØ¯"""
        if "movie" in content.lower() or "film" in content.lower():
            return "movies"
        elif "series" in content.lower() or "tv" in content.lower():
            return "tv_series"
        elif "anime" in content.lower():
            return "anime"
        elif "live" in content.lower():
            return "live_tv"
        else:
            return "general"
    
    def extract_provider_features(self, content: str) -> List[str]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…Ø²ÙˆØ¯"""
        features = []
        
        if "search" in content.lower():
            features.append("search")
        if "quality" in content.lower():
            features.append("quality_selection")
        if "subtitle" in content.lower():
            features.append("subtitles")
        if "dubbed" in content.lower():
            features.append("dubbed")
        if "live" in content.lower():
            features.append("live_streaming")
        
        return features
    
    def extract_domains(self, content: str) -> List[str]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†Ø·Ø§Ù‚Ø§Øª"""
        domains = re.findall(r'["\'](https?://[^"\']+)["\']', content)
        return list(set(domains))
    
    def categorize_providers(self, providers: List[Dict]) -> Dict[str, int]:
        """ØªØµÙ†ÙŠÙ Ø§Ù„Ù…Ø²ÙˆØ¯ÙŠÙ† Ø­Ø³Ø¨ Ø§Ù„Ù†ÙˆØ¹"""
        categories = {}
        for provider in providers:
            provider_type = provider.get("type", "unknown")
            categories[provider_type] = categories.get(provider_type, 0) + 1
        return categories
    
    def analyze_extractors(self):
        """ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬Ø§Øª"""
        print("ðŸ”§ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬Ø§Øª...")
        
        extractors = []
        
        # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬Ø§Øª
        extractor_files = list(self.root_dir.rglob("*Extractor*.kt")) + \
                         list(self.root_dir.rglob("*Extractor*.java")) + \
                         list(self.root_dir.rglob("*extractor*.py"))
        
        for file_path in extractor_files:
            try:
                with open(file_path, "r", encoding="utf-8") as f:
                    content = f.read()
                    
                    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬
                    extractor_info = {
                        "file": str(file_path),
                        "name": file_path.stem,
                        "type": self.detect_extractor_type(content),
                        "supported_sites": self.extract_supported_sites(content),
                        "features": self.extract_extractor_features(content)
                    }
                    
                    extractors.append(extractor_info)
                    
            except (UnicodeDecodeError, PermissionError):
                continue
        
        self.stats["extractors"] = {
            "total_count": len(extractors),
            "extractors": extractors,
            "by_type": self.categorize_extractors(extractors)
        }
    
    def detect_extractor_type(self, content: str) -> str:
        """Ø§ÙƒØªØ´Ø§Ù Ù†ÙˆØ¹ Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬"""
        if "stream" in content.lower():
            return "streaming"
        elif "download" in content.lower():
            return "download"
        elif "embed" in content.lower():
            return "embed"
        else:
            return "general"
    
    def extract_supported_sites(self, content: str) -> List[str]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…ÙˆØ§Ù‚Ø¹ Ø§Ù„Ù…Ø¯Ø¹ÙˆÙ…Ø©"""
        sites = re.findall(r'["\']([^"\']*\.(com|net|org|io|co)[^"\']*)["\']', content)
        return list(set([site[0] for site in sites]))
    
    def extract_extractor_features(self, content: str) -> List[str]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬"""
        features = []
        
        if "quality" in content.lower():
            features.append("quality_detection")
        if "subtitle" in content.lower():
            features.append("subtitle_support")
        if "multiple" in content.lower():
            features.append("multiple_links")
        if "proxy" in content.lower():
            features.append("proxy_support")
        
        return features
    
    def categorize_extractors(self, extractors: List[Dict]) -> Dict[str, int]:
        """ØªØµÙ†ÙŠÙ Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬Ø§Øª Ø­Ø³Ø¨ Ø§Ù„Ù†ÙˆØ¹"""
        categories = {}
        for extractor in extractors:
            extractor_type = extractor.get("type", "unknown")
            categories[extractor_type] = categories.get(extractor_type, 0) + 1
        return categories
    
    def analyze_quality(self):
        """ØªØ­Ù„ÙŠÙ„ Ø¬ÙˆØ¯Ø© Ø§Ù„Ù…Ø´Ø±ÙˆØ¹"""
        print("â­ ØªØ­Ù„ÙŠÙ„ Ø¬ÙˆØ¯Ø© Ø§Ù„Ù…Ø´Ø±ÙˆØ¹...")
        
        quality_score = 0
        quality_factors = []
        
        # ÙˆØ¬ÙˆØ¯ Ù…Ù„ÙØ§Øª Ù…Ù‡Ù…Ø©
        important_files = [
            ("README.md", "ØªÙˆØ«ÙŠÙ‚ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹"),
            ("LICENSE", "Ø§Ù„ØªØ±Ø®ÙŠØµ"),
            ("CODE_OF_CONDUCT.md", "Ù…Ø¹Ø§ÙŠÙŠØ± Ø§Ù„Ø³Ù„ÙˆÙƒ"),
            ("CONTRIBUTING.md", "Ø¯Ù„ÙŠÙ„ Ø§Ù„Ù…Ø³Ø§Ù‡Ù…Ø©"),
            ("SECURITY.md", "Ø³ÙŠØ§Ø³Ø© Ø§Ù„Ø£Ù…Ø§Ù†"),
            (".gitignore", "Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Git"),
            ("package.json", "Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Node.js"),
            ("Makefile", "Ø£ÙˆØ§Ù…Ø± Ø§Ù„Ø¨Ù†Ø§Ø¡"),
            ("Dockerfile", "Ø­Ø§ÙˆÙŠØ© Docker"),
            ("docker-compose.yml", "Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Docker Compose")
        ]
        
        for filename, description in important_files:
            if (self.root_dir / filename).exists():
                quality_score += 10
                quality_factors.append(f"âœ… {description} Ù…ÙˆØ¬ÙˆØ¯")
            else:
                quality_factors.append(f"âŒ {description} Ù…ÙÙ‚ÙˆØ¯")
        
        # ÙˆØ¬ÙˆØ¯ Ù…Ø¬Ù„Ø¯Ø§Øª Ù…Ù‡Ù…Ø©
        important_dirs = [
            ("docs", "Ø§Ù„ØªÙˆØ«ÙŠÙ‚"),
            ("scripts", "Ø§Ù„Ø³ÙƒØ±Ø¨ØªØ§Øª"),
            ("tests", "Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª"),
            ("reports", "Ø§Ù„ØªÙ‚Ø§Ø±ÙŠØ±")
        ]
        
        for dirname, description in important_dirs:
            if (self.root_dir / dirname).exists():
                quality_score += 5
                quality_factors.append(f"âœ… Ù…Ø¬Ù„Ø¯ {description} Ù…ÙˆØ¬ÙˆØ¯")
            else:
                quality_factors.append(f"âŒ Ù…Ø¬Ù„Ø¯ {description} Ù…ÙÙ‚ÙˆØ¯")
        
        # ÙˆØ¬ÙˆØ¯ Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª
        test_files = list(self.root_dir.rglob("*test*.py")) + \
                    list(self.root_dir.rglob("*Test*.kt")) + \
                    list(self.root_dir.rglob("*test*.java"))
        
        if test_files:
            quality_score += 15
            quality_factors.append(f"âœ… {len(test_files)} Ù…Ù„Ù Ø§Ø®ØªØ¨Ø§Ø± Ù…ÙˆØ¬ÙˆØ¯")
        else:
            quality_factors.append("âŒ Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…Ù„ÙØ§Øª Ø§Ø®ØªØ¨Ø§Ø±")
        
        # ÙˆØ¬ÙˆØ¯ CI/CD
        github_workflows = list((self.root_dir / ".github" / "workflows").rglob("*.yml")) if (self.root_dir / ".github" / "workflows").exists() else []
        
        if github_workflows:
            quality_score += 10
            quality_factors.append(f"âœ… {len(github_workflows)} Ø¹Ù…Ù„ÙŠØ© CI/CD Ù…ÙˆØ¬ÙˆØ¯Ø©")
        else:
            quality_factors.append("âŒ Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¹Ù…Ù„ÙŠØ§Øª CI/CD")
        
        self.stats["quality"] = {
            "score": quality_score,
            "max_score": 150,
            "percentage": round((quality_score / 150) * 100, 2),
            "grade": self.get_quality_grade(quality_score),
            "factors": quality_factors
        }
    
    def get_quality_grade(self, score: int) -> str:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¯Ø±Ø¬Ø© Ø§Ù„Ø¬ÙˆØ¯Ø©"""
        if score >= 130:
            return "A+"
        elif score >= 110:
            return "A"
        elif score >= 90:
            return "B+"
        elif score >= 70:
            return "B"
        elif score >= 50:
            return "C+"
        elif score >= 30:
            return "C"
        else:
            return "D"
    
    def generate_report(self):
        """ØªÙˆÙ„ÙŠØ¯ ØªÙ‚Ø±ÙŠØ± Ø´Ø§Ù…Ù„"""
        print("ðŸ“ ØªÙˆÙ„ÙŠØ¯ Ø§Ù„ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ...")
        
        report = {
            "project_summary": {
                "name": self.stats["project_info"].get("name", "CloudStream Extensions Arabic"),
                "version": self.stats["project_info"].get("version", "2.0.0"),
                "total_files": self.stats["files"].get("total_files", 0),
                "total_size_mb": self.stats["files"].get("total_size_mb", 0),
                "quality_score": self.stats["quality"].get("score", 0),
                "quality_percentage": self.stats["quality"].get("percentage", 0),
                "quality_grade": self.stats["quality"].get("grade", "N/A")
            },
            "code_statistics": {
                "total_lines": self.stats["code"].get("total_lines", 0),
                "total_files": self.stats["code"].get("total_files", 0),
                "languages": list(self.stats["code"].get("languages", {}).keys())
            },
            "providers_statistics": {
                "total_providers": self.stats["providers"].get("total_count", 0),
                "by_type": self.stats["providers"].get("by_type", {})
            },
            "extractors_statistics": {
                "total_extractors": self.stats["extractors"].get("total_count", 0),
                "by_type": self.stats["extractors"].get("by_type", {})
            },
            "recommendations": self.generate_recommendations()
        }
        
        return report
    
    def generate_recommendations(self) -> List[str]:
        """ØªÙˆÙ„ÙŠØ¯ ØªÙˆØµÙŠØ§Øª Ù„Ù„ØªØ­Ø³ÙŠÙ†"""
        recommendations = []
        
        # ØªÙˆØµÙŠØ§Øª Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø¬ÙˆØ¯Ø©
        quality_score = self.stats["quality"].get("score", 0)
        if quality_score < 100:
            recommendations.append("ðŸŽ¯ ØªØ­Ø³ÙŠÙ† Ø¬ÙˆØ¯Ø© Ø§Ù„Ù…Ø´Ø±ÙˆØ¹ Ù…Ù† Ø®Ù„Ø§Ù„ Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…ÙÙ‚ÙˆØ¯Ø©")
        
        # ØªÙˆØµÙŠØ§Øª Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª
        test_files = list(self.root_dir.rglob("*test*.py")) + \
                    list(self.root_dir.rglob("*Test*.kt")) + \
                    list(self.root_dir.rglob("*test*.java"))
        if len(test_files) < 5:
            recommendations.append("ðŸ§ª Ø²ÙŠØ§Ø¯Ø© ØªØºØ·ÙŠØ© Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª")
        
        # ØªÙˆØµÙŠØ§Øª Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„ØªÙˆØ«ÙŠÙ‚
        if not (self.root_dir / "docs" / "API.md").exists():
            recommendations.append("ðŸ“š Ø¥Ø¶Ø§ÙØ© ØªÙˆØ«ÙŠÙ‚ API")
        
        # ØªÙˆØµÙŠØ§Øª Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ CI/CD
        github_workflows = list((self.root_dir / ".github" / "workflows").rglob("*.yml")) if (self.root_dir / ".github" / "workflows").exists() else []
        if len(github_workflows) < 3:
            recommendations.append("ðŸ”„ Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ù…Ø²ÙŠØ¯ Ù…Ù† Ø¹Ù…Ù„ÙŠØ§Øª CI/CD")
        
        return recommendations
    
    def save_stats(self):
        """Ø­ÙØ¸ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª"""
        print("ðŸ’¾ Ø­ÙØ¸ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª...")
        
        # Ø­ÙØ¸ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„ÙƒØ§Ù…Ù„Ø©
        stats_file = self.root_dir / "reports" / "project_stats.json"
        stats_file.parent.mkdir(exist_ok=True)
        
        with open(stats_file, "w", encoding="utf-8") as f:
            json.dump(self.stats, f, ensure_ascii=False, indent=2)
        
        # Ø­ÙØ¸ Ø§Ù„ØªÙ‚Ø±ÙŠØ±
        report = self.generate_report()
        report_file = self.root_dir / "reports" / "project_report.json"
        
        with open(report_file, "w", encoding="utf-8") as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        # Ø­ÙØ¸ ØªÙ‚Ø±ÙŠØ± Markdown
        markdown_file = self.root_dir / "reports" / "project_stats.md"
        self.save_markdown_report(report, markdown_file)
        
        print_success(f"âœ… ØªÙ… Ø­ÙØ¸ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª ÙÙŠ: {stats_file}")
        print_success(f"âœ… ØªÙ… Ø­ÙØ¸ Ø§Ù„ØªÙ‚Ø±ÙŠØ± ÙÙŠ: {report_file}")
        print_success(f"âœ… ØªÙ… Ø­ÙØ¸ ØªÙ‚Ø±ÙŠØ± Markdown ÙÙŠ: {markdown_file}")
    
    def save_markdown_report(self, report: Dict, file_path: Path):
        """Ø­ÙØ¸ ØªÙ‚Ø±ÙŠØ± Markdown"""
        markdown_content = f"""# ðŸ“Š Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª CloudStream Extensions Arabic

ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ù‡Ø°Ø§ Ø§Ù„ØªÙ‚Ø±ÙŠØ± ÙÙŠ: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}

## ðŸŽ¯ Ù…Ù„Ø®Øµ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹

| Ø§Ù„Ù…Ù‚ÙŠØ§Ø³ | Ø§Ù„Ù‚ÙŠÙ…Ø© |
|---------|--------|
| Ø§Ù„Ø§Ø³Ù… | {report["project_summary"]["name"]} |
| Ø§Ù„Ø¥ØµØ¯Ø§Ø± | {report["project_summary"]["version"]} |
| Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ù„ÙØ§Øª | {report["project_summary"]["total_files"]} |
| Ø­Ø¬Ù… Ø§Ù„Ù…Ø´Ø±ÙˆØ¹ | {report["project_summary"]["total_size_mb"]} MB |
| Ø¯Ø±Ø¬Ø© Ø§Ù„Ø¬ÙˆØ¯Ø© | {report["project_summary"]["quality_grade"]} ({report["project_summary"]["quality_percentage"]}%) |

## ðŸ’» Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„ÙƒÙˆØ¯

| Ø§Ù„Ù…Ù‚ÙŠØ§Ø³ | Ø§Ù„Ù‚ÙŠÙ…Ø© |
|---------|--------|
| Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø³Ø·ÙˆØ± Ø§Ù„ÙƒÙˆØ¯ | {report["code_statistics"]["total_lines"]:,} |
| Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ù…Ù„ÙØ§Øª Ø§Ù„ÙƒÙˆØ¯ | {report["code_statistics"]["total_files"]} |
| Ø§Ù„Ù„ØºØ§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…Ø© | {', '.join(report["code_statistics"]["languages"])} |

## ðŸ“º Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ù…Ø²ÙˆØ¯ÙŠÙ†

| Ø§Ù„Ù…Ù‚ÙŠØ§Ø³ | Ø§Ù„Ù‚ÙŠÙ…Ø© |
|---------|--------|
| Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ù…Ø²ÙˆØ¯ÙŠÙ† | {report["providers_statistics"]["total_providers"]} |
| Ø§Ù„ØªØµÙ†ÙŠÙØ§Øª | {', '.join(report["providers_statistics"]["by_type"].keys())} |

## ðŸ”§ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬Ø§Øª

| Ø§Ù„Ù…Ù‚ÙŠØ§Ø³ | Ø§Ù„Ù‚ÙŠÙ…Ø© |
|---------|--------|
| Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬Ø§Øª | {report["extractors_statistics"]["total_extractors"]} |
| Ø§Ù„ØªØµÙ†ÙŠÙØ§Øª | {', '.join(report["extractors_statistics"]["by_type"].keys())} |

## ðŸ“ˆ Ø§Ù„ØªÙˆØµÙŠØ§Øª

{chr(10).join(f"- {rec}" for rec in report["recommendations"])}

---
*ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ù‡Ø°Ø§ Ø§Ù„ØªÙ‚Ø±ÙŠØ± ØªÙ„Ù‚Ø§Ø¦ÙŠÙ‹Ø§ Ø¨ÙˆØ§Ø³Ø·Ø© Ù†Ø¸Ø§Ù… Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª CloudStream Extensions Arabic*
"""
        
        with open(file_path, "w", encoding="utf-8") as f:
            f.write(markdown_content)

def main():
    """Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©"""
    print("ðŸš€ Ø¨Ø¯Ø¡ ØªØ­Ù„ÙŠÙ„ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª CloudStream Extensions Arabic...")
    
    stats_generator = ProjectStats()
    
    # ØªØ´ØºÙŠÙ„ Ø§Ù„ØªØ­Ù„ÙŠÙ„Ø§Øª
    stats_generator.analyze_project_info()
    stats_generator.analyze_files()
    stats_generator.analyze_code()
    stats_generator.analyze_providers()
    stats_generator.analyze_extractors()
    stats_generator.analyze_quality()
    
    # Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
    stats_generator.save_stats()
    
    # Ø·Ø¨Ø§Ø¹Ø© Ù…Ù„Ø®Øµ
    report = stats_generator.generate_report()
    
    print("\n" + "="*60)
    print("ðŸŽ‰ ØªÙ… Ø¥ÙƒÙ…Ø§Ù„ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø¨Ù†Ø¬Ø§Ø­!")
    print(f"ðŸ“Š Ø¯Ø±Ø¬Ø© Ø§Ù„Ø¬ÙˆØ¯Ø©: {report['project_summary']['quality_grade']} ({report['project_summary']['quality_percentage']}%)")
    print(f"ðŸ“ Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ù…Ù„ÙØ§Øª: {report['project_summary']['total_files']}")
    print(f"ðŸ’» Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø³Ø·ÙˆØ± Ø§Ù„ÙƒÙˆØ¯: {report['code_statistics']['total_lines']:,}")
    print(f"ðŸ“º Ø§Ù„Ù…Ø²ÙˆØ¯ÙŠÙ†: {report['providers_statistics']['total_providers']}")
    print(f"ðŸ”§ Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬Ø§Øª: {report['extractors_statistics']['total_extractors']}")
    print("="*60)

if __name__ == "__main__":
    main()